* natsukashii 懐かしい

*natsukashii* -- /nostalgic, reminiscent of good memories, something that is yearned for./

** About

Natsukashii is an automated web archive scraper for fanfiction.net which is
designed to find stories which have been deleted from the site a long, long time
ago, to save them from being lost forever.

This is a pet project. I made it because /I/ care about some of these stories, and
becauce I remember them from decades ago and want to read them again. If you
like this and find it useful as well, that's awesome.

This project is very much a work in progress and will change all the time (or
whenever I have time working on it, at least).


** Warning! Here be Bugs!

This tool isn't battle-tested. Running it against the various APIs does produce
errors now and then. I strongly recommend running it inside a REPL so you can
fix these errors on the go.

You can also better interact with the individual functions that way. This is
written in Common Lisp for a reason.

*** Known issues
- Most stories have only one archived chapter. This is mostly chapter 1 but it
  might be a later one, too. Because each chapter is its own archived memento,
  they all get processed as single files. So you will have =Story1-Ch1= and
  =Story1-Ch2= as two files.
- On a related note, stories that only have one chapter and are pulled through
  the new scraper (the one that processes mementos from 2002 and later) will be
  tagged with =Ch1=. This is because ff.net does tag it like that internally and
  there's really no clean way of detecting this (other than analyzing the DOM).
- The scraper might miss stories if the extraction of title and/or categories
  fails. This is usually the case when the memento only contains "Story not
  found" (in which case it's supposed to not pull it!) but it could have false
  positives too.
- Some stories contain invalid XML chracters. Note that most of them were
  written before everything went Unicode. The parser tries its best to just work
  with those (you might get weird black blocks in the output sometimes, though),
  but sometimes it can't. In those cases, the story won't be fetched.


** How to use

Assuming you cloned the repo into any of the folders inside Quicklisp's
~*local-project-directories*~, this should be enough to load it:

#+begin_src lisp
  (ql:quickload :natsukashii)
  (in-package :natsukashii)
#+end_src

You have then two different sets of scrapers, ~old--~ and ~new--~. This is because
the whole website structure of ff.net changed in September 2001 and you
essentially have to process any memento from before that different.

*** Scraper for old stories (pre September 2001)

To use the old scraper you can do this:

#+begin_src lisp
  (old--find-archived-stories)
#+end_src

This will pull *everything* that was archived under the old website (pre
2001-11). This isn't too much, about 3000 pages give or take, so pulling them
all is possible in one go.

For more fine grained controll you can use these:

#+begin_src lisp
  (old--fetch-stories-in-category "anime")
#+end_src

Where instead of "anime" you can use any of the old category names, which is one
of:
- "anime"
- "books"
- "cartoons"
- "comics"
- "crossovers"
- "games"
- "misc"
- "movies"
- "musicgroups"
- "originals"
- "poetries"
- "tvshows"

The function is concurrent, and will by default run eight threads in
parallel. You can change this value by using the keyword argument ~:threads~, but
please keep in mind that firing too many requests at one is a strain on the API.

*** Scraper for newer stories (Septemter 2001 and later)

The new scraper works a bit different. You use it like this:

#+begin_src lisp
  (new--fetch-all-stories :from 0 :process 100)
#+end_src

This will then first pull a list of *all* available stories from the CDX API
(which should be over 60,000), and then attempt to download 100 of them,
starting at index 0. The function will return the index at which it stopped
pulling, so to continue later, you can simply call the function again with ~:from~
set to whatever was returned in the last iteration to continue.

That way you don't have to download all of them in one go, which might take a
very long time, and is error-prone b/c sometimes the Wayback API is very slow
and unresponsive (but it's free, so no bad feelings about it)!

This function also is concurrent and runs eight threads by default. Again, you
can change this with the ~:threads~ keyword.

The initial CDX response will be saved in the special variable
~*all-stories-cdx-response*~ which is available to you as well. If you want to
access one story specifically, you can call ~nth~ on that list to get the memento
out. It'll contain the URL that the Wayback Machine wants so you can put it in
there and view the memento online as well (e.g. to check a story that has failed
to download).

You can also attempt to fetch a single story by using the memento for it:

#+begin_src lisp
  (new--fetch-story memento)
#+end_src


** A word of advice

Please respect that this API is free and don't fire thousands of requests
against it. Be respectful!
